{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usdkfzz_Qv7-"
      },
      "source": [
        "\n",
        "# Part I: Download dataset, select subset and preprocess data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean previous data. Uncomment if data become messed up.\n",
        "# !rm -rf LibriSpeech minilibri dev-clean-2*"
      ],
      "metadata": {
        "id": "oKMUG50vwDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers jiwer"
      ],
      "metadata": {
        "id": "2G2IT0pM5LcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.openslr.org/resources/31/dev-clean-2.tar.gz"
      ],
      "metadata": {
        "id": "XvI3BniM5ONy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofejdX2ezJdI"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!tar xvf dev-clean-2.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4RKC_8azXeu"
      },
      "outputs": [],
      "source": [
        "import jiwer\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import subprocess\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset, Audio, load_dataset\n",
        "from pathlib import Path\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DONE: Set the seed to your date of birth (DDMM) e.g. 0101 for 1st of January\n",
        "SEED = int(\"2406\") # 24th of June\n",
        "\n",
        "random.seed(SEED)  # For more reproducible results"
      ],
      "metadata": {
        "id": "g3Vg1fFx8B8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTNo1pll0_vg"
      },
      "outputs": [],
      "source": [
        "LIBRISPEECH_DIR = \"LibriSpeech/dev-clean-2\"\n",
        "SUBSET_DIR = \"minilibri\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzScGQTjHaRk"
      },
      "outputs": [],
      "source": [
        "def find_speakers(librispeech_dir):\n",
        "  \"\"\"Returns a list of all speakers\"\"\"\n",
        "  return os.listdir(librispeech_dir)\n",
        "\n",
        "\n",
        "def find_files(librispeech_dir, speaker, extension):\n",
        "  \"\"\"Finds files with specific extension for a specific speaker\"\"\"\n",
        "  return [\n",
        "    f.as_posix()\n",
        "    for f in (Path(librispeech_dir) / speaker).glob(f\"**/*.{extension}\")\n",
        "  ]\n",
        "\n",
        "\n",
        "def select_random_subset(utterance_ids, num=5):\n",
        "  \"\"\"Select specified number of random items from an input list\"\"\"\n",
        "  utts = list(utterance_ids)\n",
        "  random.shuffle(utts)\n",
        "  return utts[:num]\n",
        "\n",
        "\n",
        "def merge_transcriptions(transcription_dicts):\n",
        "  \"\"\"Takes a list of transcription dictionaries, and merges them\n",
        "  into a single dictionary\n",
        "  \"\"\"\n",
        "  merged = {}\n",
        "  for d in transcription_dicts:\n",
        "    merged.update(d)\n",
        "  return merged\n",
        "\n",
        "\n",
        "def get_utterance_id_from_path(path):\n",
        "  return Path(path).stem\n",
        "\n",
        "\n",
        "def get_speaker_from_utterance_id(utt_id):\n",
        "  return utt_id.split(\"-\")[0]\n",
        "\n",
        "\n",
        "def convert_flac_to_wav(flac_path, output_dir):\n",
        "    \"\"\"\n",
        "    Converts a FLAC audio file to WAV format using ffmpeg, with a sample rate of 16000 Hz and mono audio.\n",
        "\n",
        "    Parameters:\n",
        "    flac_path (str): The path to the input FLAC file.\n",
        "    output_dir (str): The directory where the output WAV file will be saved.\n",
        "\n",
        "    The output WAV file is saved in a subdirectory of `output_dir` matching the input file's immediate parent directory,\n",
        "    and the file name is preserved with the `.wav` extension.\n",
        "    \"\"\"\n",
        "    # Extract the filename without extension and the directory name\n",
        "    utt_id = get_utterance_id_from_path(flac_path)\n",
        "    output_file = f\"{output_dir}/{utt_id}.wav\"\n",
        "\n",
        "    # Construct the ffmpeg command\n",
        "    command = [\n",
        "        'ffmpeg',\n",
        "        '-i', flac_path, # Input file\n",
        "        '-ar', '16000',  # Set audio sample rate to 16000 Hz\n",
        "        '-ac', '1',      # Set audio channels to 1 (mono)\n",
        "        output_file\n",
        "    ]\n",
        "\n",
        "    # Execute the ffmpeg command\n",
        "    try:\n",
        "        subprocess.run(command, check=True)\n",
        "        print(f\"Conversion successful, output file saved to {output_file}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error during conversion: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PFJ_JBC0HXi"
      },
      "outputs": [],
      "source": [
        "def find_flac_files(librispeech_dir, speaker):\n",
        "  \"\"\"find all flac files for a specific speaker (use find_files)\"\"\"\n",
        "  return find_files(librispeech_dir, speaker, \"flac\")\n",
        "\n",
        "\n",
        "def find_transcriptions(librispeech_dir, speaker):\n",
        "  \"\"\"find all trans.txt files for a specific speaker\"\"\"\n",
        "  return find_files(librispeech_dir, speaker, \"trans.txt\")\n",
        "\n",
        "\n",
        "def convert_flac_list_to_dict(flac_files):\n",
        "  \"\"\"Takes a list of flac files and returns a dict with all files in the format\n",
        "  {\n",
        "    utterance_id: flac_filepath\n",
        "  }\n",
        "  The utterance id can be extracted from the filepath using Path(filepath).stem\n",
        "  \"\"\"\n",
        "\n",
        "  return {Path(filepath).stem: filepath for filepath in flac_files}  #creates a dictionary with utt_id in full flac path\n",
        "\n",
        "\n",
        "def read_transcription(transcription_file, lowercase=False):\n",
        "  \"\"\"Read a transcription file.\n",
        "  Open file, read lines and convert to lower case if lowercase == True.\n",
        "  Returns a dictionary with all transcriptions in the format:\n",
        "  {\n",
        "    utterance_id: transcription_text\n",
        "  }\n",
        "  \"\"\"\n",
        "  transcription_dict = {} #open a blank dict to add then utt_id and trans_txt\n",
        "  with open(transcription_file, 'r', encoding='utf-8') as file: #open and read the trans_file\n",
        "    for line in file:  #read each line in file\n",
        "      split_text = line.strip().split() #split line into list\n",
        "      utterance_id = split_text[0]  # take only the first element which is the ID number\n",
        "      transcription_text = \" \".join(split_text[1:])   # after the ID I take the rest of the trans_text\n",
        "\n",
        "      # save in the afforementioned dict\n",
        "      transcription_dict[utterance_id] = transcription_text\n",
        "\n",
        "  return transcription_dict\n",
        "\n",
        "def create_librispeech_subset(librispeech_dir):\n",
        "  \"\"\"Selects a subset of 5 utterance for each speaker and returns 2 dictionaries\n",
        "  1. flac_dict: maps the selected utterance ids to the corresponding flac files\n",
        "  2. transcription_dict: maps the selected utterance ids to the corresponding\n",
        "     transcriptions\n",
        "  How:\n",
        "    a. Get all speakers\n",
        "    b. For each speaker\n",
        "      i. get all flac files for that speaker\n",
        "      ii. read all transcriptions for that speaker\n",
        "      iii. select a random subset of utterance_ids\n",
        "      iv. populate selected_flacs dict with the flacs corresponding to the\n",
        "        selected utterance ids\n",
        "      v. populated selected_transcriptions dic twith the transcriptions\n",
        "        corresponding to the selected utterance ids\n",
        "    c. Return selected_flacs, selected_transcriptions\n",
        "  \"\"\"\n",
        "  speakers = find_speakers(librispeech_dir) # take a list of all speakers\n",
        "  selected_flac_files = {} # make a dict for then storing of the flac files\n",
        "  selected_transcriptions = {} # make a dict for the selected transcriptions then\n",
        "\n",
        "  for speaker in speakers:\n",
        "    flac_files = find_flac_files(librispeech_dir, speaker) #get all the flac files for each speaker\n",
        "    transcription_files = find_transcriptions(librispeech_dir, speaker) #get all trans texts for each speaker\n",
        "\n",
        "    flac_dict = convert_flac_list_to_dict(flac_files) #convert flac list to dict\n",
        "    transcription_dicts = [read_transcription(file) for file in transcription_files] #read all the trans.txt files\n",
        "    transcription_dict = merge_transcriptions(transcription_dicts) #merge all trans dicts into one dict\n",
        "\n",
        "    ids = list(set(flac_dict) & set(transcription_dict)) # take the IDs that are in both flac files and trans txts\n",
        "    selected_ids = select_random_subset(ids, num=5) # take 5 random utterance IDs\n",
        "\n",
        "    for utterance_id in selected_ids:\n",
        "        selected_flac_files[utterance_id] = flac_dict[utterance_id] # save selected flac paths\n",
        "\n",
        "    for utterance_id in selected_ids:\n",
        "        selected_transcriptions[utterance_id] = transcription_dict[utterance_id] # save corresponding transcriptions\n",
        "\n",
        "  return selected_flac_files, selected_transcriptions\n",
        "\n",
        "def write_trans_txt(transcription_dict, output_file):\n",
        "  \"\"\"Write a transcription dict in a output trans.txt file.\n",
        "  Lines in the format:\n",
        "    utterance_id this is the transcript\n",
        "  \"\"\"\n",
        "  with open(output_file, 'w', encoding='utf-8') as f: #open output .txt file for writing\n",
        "    for utterance_id, transcription_text in transcription_dict.items():\n",
        "      f.write(f\"{utterance_id} {transcription_text}\\n\") #write each line based on the commented format\n",
        "\n",
        "\n",
        "def write_subset_to_disk(flac_dict, transcription_dict, output_dir):\n",
        "  \"\"\"\n",
        "  1. Create output_dir\n",
        "  2. For each file in flac_dict convert it to wav and write in {output_dir}\n",
        "  3. Write the transcription_dict in the file {output_dir}/transcipt.trans.txt\n",
        "  \"\"\"\n",
        "  os.makedirs(output_dir, exist_ok=True) #create output directory\n",
        "\n",
        "  for utterance_id, flac_path in flac_dict.items():\n",
        "    convert_flac_to_wav(flac_path, output_dir) # convert the selected flac files to WAV files\n",
        "\n",
        "  output_transcription_path = f\"{output_dir}/transcript.trans.txt\" # create the output trans file path\n",
        "  write_trans_txt(transcription_dict, output_transcription_path) # write the trans dict in the file path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H4tw0szRyCov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxs35kOFJ8_d"
      },
      "outputs": [],
      "source": [
        "# Call implemented functions\n",
        "selected_flacs, selected_transcripts = create_librispeech_subset(LIBRISPEECH_DIR)\n",
        "write_subset_to_disk(selected_flacs, selected_transcripts, SUBSET_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vDphR52RElb"
      },
      "source": [
        "# Part II: Convert data to huggingface dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7vvZ75FTLfq"
      },
      "outputs": [],
      "source": [
        "def make_wav_dataset(subset_dir):\n",
        "  \"\"\"Create a huggingface dataset based on a list of wav files\n",
        "  1. Use glob to get a list of wav files from the subset_dir\n",
        "  2. Use Dataset.from_dict to create a dataset from the list of wav files\n",
        "  3. Use cast_column to cast the \"audio\" column to Audio()\n",
        "  \"\"\"    # I altered the provided code, because the previous one used Path.glob() in Dataset.from_dict and didn t work - it returned an iterator and not a list with file paths !\n",
        "  list_of_wavs = [str(p) for p in Path(subset_dir).glob(\"*.wav\")]  # convert paths to strings\n",
        "  dataset = Dataset.from_dict({\"audio\": list_of_wavs})  # create a huggingface dataset\n",
        "  dataset = dataset.cast_column(\"audio\", Audio())      #cast the column to Audio type\n",
        "  return dataset\n",
        "\n",
        "def augment_dataset_with_ids(dataset):\n",
        "  \"\"\"Create a column \"id\" for each sample containing the corresponding\n",
        "  utterance id.\n",
        "  Hint: Use dataset.map and get_utterance_id_from_path\n",
        "  \"\"\"\n",
        "  def extract_id(sample):\n",
        "    path = sample[\"audio\"][\"path\"] #take the path of audio file\n",
        "    utterance_id = get_utterance_id_from_path(path) #extact the utterance_id\n",
        "    return {\"id\": utterance_id} # make it a dict to add it as a column\n",
        "\n",
        "  return dataset.map(extract_id) # apply the function to all dataset rows\n",
        "\n",
        "\n",
        "def augment_dataset_with_transcriptions(dataset, subset_dir):\n",
        "  \"\"\"Create a column \"text\" for each sample containing the sample's\n",
        "  transcription.\n",
        "  Hint: use read_transcription to read the transcript.trans.txt and dataset.map\n",
        "  to add the transcription to each sample based on the \"id\" column\n",
        "  \"\"\"\n",
        "  transcript_path = Path(subset_dir) / \"transcript.trans.txt\"  # path to the transcription file\n",
        "  trans_dict = read_transcription(transcript_path) # read the file in a dict\n",
        "\n",
        "  def add_transcription(sample):\n",
        "    utt_id = sample [\"id\"] # take the ID of the current sample\n",
        "    return {\"text\": trans_dict[utt_id]}  # add the corresponding transcription as new column with id\n",
        "\n",
        "  return dataset.map(add_transcription) # apply the function on the dataset\n",
        "\n",
        "def augment_dataset_with_speakers(dataset):\n",
        "  \"\"\"Create a column \"speaker\" for each sample containing the sample's speaker\n",
        "  Hint: Use dataset.map and get_speaker_from_utterance_id\n",
        "  \"\"\"\n",
        "  def add_speaker(sample): # make a function to add a new column \"speaker\" connected to their ID\n",
        "    utt_id = sample[\"id\"] # take the utterance id\n",
        "    speaker = get_speaker_from_utterance_id(utt_id)  # extract speaker id from it\n",
        "    return {\"speaker\": speaker}  # add the speaker ID as new column\n",
        "\n",
        "  return dataset.map(add_speaker) # apply to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtGMc98iW5o1"
      },
      "outputs": [],
      "source": [
        "def make_dataset(subset_dir):\n",
        "  \"\"\"Create the minilibrispeech dataset using the helper functions\"\"\"\n",
        "  dataset = make_wav_dataset(subset_dir)\n",
        "  dataset = augment_dataset_with_ids(dataset)\n",
        "  dataset = augment_dataset_with_transcriptions(dataset, subset_dir)\n",
        "  dataset = augment_dataset_with_speakers(dataset)\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZJRtHOXWw8I"
      },
      "source": [
        "# Part III: Augment dataset with noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU59r16AcBYM"
      },
      "outputs": [],
      "source": [
        "def download_noise_dataset():\n",
        "  \"\"\"Download a dataset with noise recordings from huggingface hub and\n",
        "  downsample to 16000 Hz\n",
        "  \"\"\"\n",
        "  noise_dataset = load_dataset(\"Nexdata/Scene_Noise_Data\", split=\"train\")\n",
        "  # Downsample at 16 kHz\n",
        "  noise_dataset = noise_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "  return noise_dataset\n",
        "\n",
        "\n",
        "def adjust_noise_length(noise, target_length):\n",
        "  \"\"\"Adjust the length of the noise to match the target length.\n",
        "  Args:\n",
        "      noise (numpy.ndarray): Noise signal.\n",
        "      target_length (int): Desired length of the noise signal.\n",
        "  Returns:\n",
        "      numpy.ndarray: Adjusted noise signal.\n",
        "  \"\"\"\n",
        "  if len(noise) > target_length:\n",
        "    return noise[:target_length]\n",
        "  elif len(noise) < target_length:\n",
        "    repeat_count = target_length // len(noise) + 1\n",
        "    return np.tile(noise, repeat_count)[:target_length]\n",
        "  return noise\n",
        "\n",
        "\n",
        "def snr_db_to_linear(snr_db):\n",
        "  \"\"\"\n",
        "  Convert SNR from decibels (dB) to a linear scale.\n",
        "  Args:\n",
        "      snr_db (float): SNR value in decibels.\n",
        "  Returns:\n",
        "      float: Actual SNR as a ratio.\n",
        "  \"\"\"\n",
        "  return 10 ** (snr_db / 10)\n",
        "\n",
        "\n",
        "def snr_linear_to_db(snr):\n",
        "  \"\"\"\n",
        "  Convert SNR from linear tp decibels (dB)\n",
        "  Args:\n",
        "      snr (float): SNR value.\n",
        "  Returns:\n",
        "      float: SNR in decibels (DB).\n",
        "  \"\"\"\n",
        "  return 10 * np.log10(snr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei2mBgEFes4Y"
      },
      "outputs": [],
      "source": [
        "def calculate_power(signal):\n",
        "  \"\"\"\n",
        "  Calculate the power of a signal. The power is calculated as the sum of\n",
        "  the squared signal.\n",
        "  \"\"\"\n",
        "  return (signal * signal).sum()\n",
        "\n",
        "\n",
        "def calculate_snr(signal, noise):\n",
        "  \"\"\"\n",
        "  Calculate the signal-to-noise ratio (SNR) for given signal and noise arrays.\n",
        "  1. Calculate the power of the signal and the noise\n",
        "  2. Return the ratio signal_power / noise_power\n",
        "  \"\"\"\n",
        "  signal_power = calculate_power(signal)  # calculate signal power\n",
        "  noise_power = calculate_power(noise)    # calculate noise_power\n",
        "\n",
        "  return signal_power / noise_power  # return the ratio signal_power / noise_power\n",
        "\n",
        "def calculate_noise_coefficient(signal, noise, desired_snr_db):\n",
        "  \"\"\"\n",
        "  Calculate the noise coefficient required to scale the noise to achieve a\n",
        "  desired SNR in dB using the signal power calculation function.\n",
        "\n",
        "  1. Calculate the signal and the noise power\n",
        "  2. Convert the desired SNR from DB to linear scale\n",
        "  3. Calculate the target noise power as signal_power / desired_snr\n",
        "  4. Calculate the coefficient as the square root of\n",
        "     target_noise_power / noise_power\n",
        "  \"\"\"\n",
        "  signal_power = calculate_power(signal) # calculate signal power\n",
        "  noise_power = calculate_power(noise) # calculate noise power\n",
        "\n",
        "  linear_snr = snr_db_to_linear(desired_snr_db) # convert dB to linear scale\n",
        "  target_noise_power = signal_power / linear_snr # calculate the target noise power\n",
        "\n",
        "  coeff = np.sqrt(target_noise_power / noise_power) # calculate the coeff\n",
        "\n",
        "  return coeff\n",
        "\n",
        "def add_noise_to_signal(signal, noise, desired_snr_db):\n",
        "  \"\"\"\n",
        "  Add noise to a signal to achieve a specified SNR.\n",
        "  1. Adjust the noise length\n",
        "  2. Calculate the noise coefficient for the target snr_db\n",
        "  3. Scale (multiply) the noise by the coefficient\n",
        "  4. Add the scaled noise to the input signal to create the noisy signal\n",
        "  5. Return the noisy signal\n",
        "  \"\"\"\n",
        "  noise = adjust_noise_length(noise, len(signal)) # adjust the noise length\n",
        "  coeff = calculate_noise_coefficient(signal, noise, desired_snr_db) #calculate noise coeff for the target snr_db\n",
        "  scale_noise = noise * coeff # scale the n oise by the coeff\n",
        "  noisy_signal = signal + scale_noise #add the scaled noise to the input signal to create the noisy signal\n",
        "  return noisy_signal\n",
        "\n",
        "def make_noisy_dataset(audio_dataset, noise_dataset, desired_snr_db):\n",
        "  \"\"\"\n",
        "  Add noise to each audio signal in the dataset to achieve a specified SNR\n",
        "  using dataset.map\n",
        "  \"\"\"\n",
        "  def process_sample(sample):\n",
        "    \"\"\"\n",
        "    1. Choose a random sample from the noise_dataset\n",
        "    2. Use add_noise_to_signal with the sample[\"audio\"][\"array\"] and the\n",
        "       random noise sample to create a noisy_signal\n",
        "    3. Set sample[\"audio\"][\"array\"] as the noisy_signal\n",
        "    \"\"\"\n",
        "    noise_example = random.choice (noise_dataset) # Choose a random sample from the noise_dataset\n",
        "    noise_array = noise_example [\"audio\"][\"array\"] # take the noise array\n",
        "\n",
        "    noisy_signal = add_noise_to_signal(sample[\"audio\"][\"array\"],noise_array,desired_snr_db)\n",
        "    # create a noisy_signal\n",
        "\n",
        "    sample[\"audio\"][\"array\"] = noisy_signal # set sample[\"audio\"][\"array\"] as the noisy_signal\n",
        "\n",
        "    return sample\n",
        "\n",
        "  dataset = audio_dataset.map(process_sample)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_clean = make_dataset(SUBSET_DIR)\n",
        "noise_dataset = download_noise_dataset()\n",
        "dataset_snr_3 = make_noisy_dataset(dataset_clean, noise_dataset, 3)\n",
        "dataset_snr_6 = make_noisy_dataset(dataset_clean, noise_dataset, 6)\n",
        "dataset_snr_9 = make_noisy_dataset(dataset_clean, noise_dataset, 9)"
      ],
      "metadata": {
        "id": "BobcpzFHAgsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experimenting listening to an audio sample to verify that the noise was added correctly - Listen to examples of the noisy data to understand how each noise level is perceived\n",
        "from IPython.display import Audio\n",
        "\n",
        "sample = dataset_snr_3[20]\n",
        "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "_r4aJuz-Gu_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset_snr_6[20]\n",
        "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "TmCRMgv2T_fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset_snr_9[20]\n",
        "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "AyEe0fRnUK19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV: Create speaker specific subsets"
      ],
      "metadata": {
        "id": "tdUM5T4qAZCn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raxr1tNLi2L9"
      },
      "outputs": [],
      "source": [
        "def filter_dataset_for_speaker(dataset, speaker):\n",
        "  \"\"\"Use dataset.filter to create a new dataset containing only the samples\n",
        "  that correspond to the speaker\n",
        "  \"\"\"\n",
        "\n",
        "  def target_speaker(sample): # make a function that returns true if the speaker of the example matches the targeted filtered speaker\n",
        "     return sample[\"speaker\"] == speaker\n",
        "\n",
        "  new_dataset = dataset.filter(target_speaker) # Use dataset.filter for new dataset with corresponding speaker\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPEAKER1 = \"1272\"\n",
        "SPEAKER2 = \"3000\"\n",
        "\n",
        "dataset_speaker_1 = filter_dataset_for_speaker(dataset_clean, SPEAKER1)\n",
        "dataset_speaker_2 = filter_dataset_for_speaker(dataset_clean, SPEAKER2)\n",
        "\n",
        "dataset_snr_3_speaker_1 = filter_dataset_for_speaker(dataset_snr_3, SPEAKER1)\n",
        "dataset_snr_3_speaker_2 = filter_dataset_for_speaker(dataset_snr_3, SPEAKER2)"
      ],
      "metadata": {
        "id": "Q2kY_B5YBBFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write this just to confirm that each dataset contains samples after filtering\n",
        "print(len(dataset_speaker_1))\n",
        "print(len(dataset_speaker_2))\n",
        "print(len(dataset_snr_3_speaker_1))\n",
        "print(len(dataset_snr_3_speaker_2))"
      ],
      "metadata": {
        "id": "362BwdbQEUlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part V: Experiments with wav2vec2"
      ],
      "metadata": {
        "id": "Pc0-a7pnx9F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WAV2VEC2_BASE = \"facebook/wav2vec2-base-960h\"\n",
        "WAV2VEC2_LARGE = \"facebook/wav2vec2-large-960h\"\n",
        "WAV2VEC2_LARGE_SELF = \"facebook/wav2vec2-large-960h-lv60-self\""
      ],
      "metadata": {
        "id": "c5K9IJImzDz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_and_processor(model_name):\n",
        "  \"\"\"Load pretrained huggingface model\"\"\"\n",
        "  model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "  processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "  return model, processor\n",
        "\n",
        "\n",
        "def calculate_wer(reference, hypothesis):\n",
        "  \"\"\"Calculate Word Error Rate between reference (ground truth) text and\n",
        "  hypothesis (predicted) text\n",
        "  \"\"\"\n",
        "  return jiwer.wer(reference, hypothesis) * 100\n",
        "\n",
        "\n",
        "def calculate_cer(reference, hypothesis):\n",
        "  \"\"\"Calculate Character Error Rate between reference (ground truth) text and\n",
        "  hypothesis (predicted) text\n",
        "  \"\"\"\n",
        "  return jiwer.cer(reference, hypothesis) * 100"
      ],
      "metadata": {
        "id": "O1nPAzPNx1cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, processor, dataset):\n",
        "  model = model.to(\"cuda\")\n",
        "  model.eval()\n",
        "\n",
        "  def map_to_pred(batch):\n",
        "    \"\"\"\n",
        "    1. Pass batch[\"audio\"][\"array\"] through the processor and return\n",
        "      pytorch (pt) tensors. Get the input values of the resulting object.\n",
        "    2. Use with torch.no_grad() to disable gradients\n",
        "      Calculate the logits by passing the input values through the model\n",
        "    3. Get the predicted ids using argmax on the logits\n",
        "    4. Get the transcription by using processor.batch_decode\n",
        "    5. Set batch[\"transcription\"] to the transcription\n",
        "    \"\"\"\n",
        "    inputs = processor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    # Pass batch[\"audio\"][\"array\"] through the processor and return pytorch (pt) tensors and apply padding for input alignment\n",
        "\n",
        "    with torch.no_grad(): # disable gradient calculation\n",
        "      logits = model(inputs.input_values.to(\"cuda\")).logits #calculate the logits from the model\n",
        "\n",
        "    predicted_ids = torch.argmax (logits, dim=-1) # get the predicted ids using argmax on the logits (get the index of max logit)\n",
        "    transcription = processor.batch_decode(predicted_ids)[0] # get the transcription\n",
        "    batch[\"transcription\"] = transcription # set batch[\"transcription\"] to the transcription\n",
        "    return batch  # return the updated batch\n",
        "\n",
        "  result = dataset.map(map_to_pred, batched=False, remove_columns=[\"audio\"])\n",
        "  WER = calculate_wer(result[\"text\"], result[\"transcription\"])\n",
        "  CER = calculate_cer(result[\"text\"], result[\"transcription\"])\n",
        "  return WER, CER"
      ],
      "metadata": {
        "id": "ZsPGzFLOzBG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiment on clean data - WAV2VEC2-BASE\n",
        "model_base, processor_base = get_model_and_processor(WAV2VEC2_BASE)\n",
        "wer_clean, cer_clean = evaluate(model_base, processor_base, dataset_clean)\n",
        "print(f\"WER - Dataset clean - wav2vec2 base: {wer_clean}\")\n",
        "print(f\"CER - Dataset clean - wav2vec2 base: {cer_clean}\")\n",
        "\n",
        "# DONE: Run the rest of the experiments for the noisy and the single speaker datasets for wav2vec2-base\n",
        "# DONE: Run the rest of the experiments for the clean, noisy, and the single speaker datasets for wav2vec2-large\n",
        "# DONE: Run the rest of the experiments for the clean, noisy, and the single speaker datasets for wav2vec2-large-self"
      ],
      "metadata": {
        "id": "X5tON3sOwioE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC2-BASE for noisy datasets (SNR 3, 6, 9)\n",
        "model_base, processor_base = get_model_and_processor(WAV2VEC2_BASE)\n",
        "wer_snr3_base, cer_snr3_base = evaluate(model_base,processor_base,dataset_snr_3)\n",
        "print(f\"WER - SNR 3 - wav2vec2 base: {wer_snr3_base}\")\n",
        "print(f\"CER - SNR 3 - wav2vec2 base: {cer_snr3_base}\")\n",
        "\n",
        "wer_snr6_base, cer_snr6_base = evaluate(model_base,processor_base,dataset_snr_6)\n",
        "print(f\"WER - SNR 6 - wav2vec2 base: {wer_snr6_base}\")\n",
        "print(f\"CER - SNR 6 - wav2vec2 base: {cer_snr6_base}\")\n",
        "\n",
        "wer_snr9_base, cer_snr9_base = evaluate(model_base,processor_base,dataset_snr_9)\n",
        "print(f\"WER - SNR 9 - wav2vec2 base: {wer_snr9_base}\")\n",
        "print(f\"CER - SNR 9 - wav2vec2 base: {cer_snr9_base}\")\n",
        "\n",
        "# WAV2VEC2-BASE for single speaker clean dataset | speakers 1 and 2\n",
        "wer_clean_speaker1_base, cer_clean_speaker1_base = evaluate(model_base, processor_base, dataset_speaker_1)\n",
        "print(f\"WER - Clean speaker 1 - wav2vec2 base: {wer_clean_speaker1_base}\")\n",
        "print(f\"CER - Clean speaker 1 - wav2vec2 base: {cer_clean_speaker1_base}\")\n",
        "\n",
        "wer_clean_speaker2_base, cer_clean_speaker2_base = evaluate(model_base, processor_base, dataset_speaker_2)\n",
        "print(f\"WER - Clean speaker 2 - wav2vec2 base: {wer_clean_speaker2_base}\")\n",
        "print(f\"CER - Clean speaker 2 - wav2vec2 base: {cer_clean_speaker2_base}\")\n",
        "\n",
        "# WAV2VEC2-BASE for single speaker noisy (SNR 3) | speakers 1 and 2\n",
        "wer_noisy_speaker1_base, cer_noisy_speaker1_base = evaluate(model_base, processor_base, dataset_snr_3_speaker_1)\n",
        "print(f\"WER - Noisy SNR 3 speaker 1- wav2vec2 base: {wer_noisy_speaker1_base}\")\n",
        "print(f\"CER - Noisy SNR 3 speaker 1- wav2vec2 base: {cer_noisy_speaker1_base}\")\n",
        "\n",
        "wer_noisy_speaker2_base, cer_noisy_speaker2_base = evaluate(model_base, processor_base, dataset_snr_3_speaker_2)\n",
        "print(f\"WER - Noisy SNR 3 speaker 2- wav2vec2 base: {wer_noisy_speaker2_base}\")\n",
        "print(f\"CER - Noisy SNR 3 speaker 2- wav2vec2 base: {cer_noisy_speaker2_base}\")"
      ],
      "metadata": {
        "id": "D1OHva4VNrqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Wav2Vec2-Large-960h\n",
        "\n",
        "model_large, processor_large = get_model_and_processor(WAV2VEC2_LARGE)\n",
        "\n",
        "# Wav2Vec2-Large for clean dataset\n",
        "wer_clean_large, cer_clean_large = evaluate(model_large, processor_large, dataset_clean)\n",
        "print(f\"WER - Clean - wav2vec2 large: {wer_clean_large}\")\n",
        "print(f\"CER - Clean - wav2vec2 large: {cer_clean_large}\")\n",
        "\n",
        "# Wav2Vec2-Large for SNR noisy dataset\n",
        "wer_snr3_large, cer_snr3_large = evaluate(model_large, processor_large, dataset_snr_3)\n",
        "print(f\"WER - SNR 3 - wav2vec2 large: {wer_snr3_large}\")\n",
        "print(f\"CER - SNR 3 - wav2vec2 large: {cer_snr3_large}\")\n",
        "\n",
        "wer_snr6_large, cer_snr6_large = evaluate(model_large, processor_large, dataset_snr_6)\n",
        "print(f\"WER - SNR 6 - wav2vec2 large: {wer_snr6_large}\")\n",
        "print(f\"CER - SNR 6 - wav2vec2 large: {cer_snr6_large}\")\n",
        "\n",
        "wer_snr9_large, cer_snr9_large = evaluate(model_large, processor_large, dataset_snr_9)\n",
        "print(f\"WER - SNR 9 - wav2vec2 large: {wer_snr9_large}\")\n",
        "print(f\"CER - SNR 9 - wav2vec2 large: {cer_snr9_large}\")\n",
        "\n",
        "#  Wav2Vec2-Large for single speaker clean dataset | speakers 1 and 2\n",
        "wer_clean_speaker1_large, cer_clean_speaker1_large = evaluate(model_large, processor_large, dataset_speaker_1)\n",
        "print(f\"WER - Clean speaker 1 - wav2vec2 large: {wer_clean_speaker1_large}\")\n",
        "print(f\"CER - Clean speaker 1 - wav2vec2 large: {cer_clean_speaker1_large}\")\n",
        "\n",
        "wer_clean_speaker2_large, cer_clean_speaker2_large = evaluate(model_large, processor_large, dataset_speaker_2)\n",
        "print(f\"WER - Clean speaker 2 - wav2vec2 large: {wer_clean_speaker2_large}\")\n",
        "print(f\"CER - Clean speaker 2 - wav2vec2 large: {cer_clean_speaker2_large}\")\n",
        "\n",
        "# WAV2VEC2-Large for single speaker noisy (SNR 3) | speakers 1 and 2\n",
        "wer_noisy_speaker1_large, cer_noisy_speaker1_large = evaluate(model_large, processor_large, dataset_snr_3_speaker_1)\n",
        "print(f\"WER - Noisy SNR 3 speaker 1- wav2vec2 large: {wer_noisy_speaker1_large}\")\n",
        "print(f\"CER - Noisy SNR 3 speaker 1- wav2vec2 large: {cer_noisy_speaker1_large}\")\n",
        "\n",
        "wer_noisy_speaker2_large, cer_noisy_speaker2_large = evaluate(model_large, processor_large, dataset_snr_3_speaker_2)\n",
        "print(f\"WER - Noisy SNR 3 speaker 2- wav2vec2 large: {wer_noisy_speaker2_large}\")\n",
        "print(f\"CER - Noisy SNR 3 speaker 2- wav2vec2 large: {cer_noisy_speaker2_large}\")"
      ],
      "metadata": {
        "id": "BE4BfSMwRqQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Wav2Vec2-Large-960h-Lv60 + Self-Training\n",
        "model_self, processor_self = get_model_and_processor(WAV2VEC2_LARGE_SELF)\n",
        "\n",
        "# Wav2Vec2-Large-Self for clean dataset\n",
        "wer_clean_self, cer_clean_self = evaluate(model_self, processor_self, dataset_clean)\n",
        "print(f\"WER - Clean - wav2vec2 large-self: {wer_clean_self}\")\n",
        "print(f\"CER - Clean - wav2vec2 large-self: {cer_clean_self}\")\n",
        "\n",
        "# Wav2Vec2-Large-Self for SNR noisy dataset\n",
        "wer_snr3_self, cer_snr3_self = evaluate(model_self, processor_self, dataset_snr_3)\n",
        "print(f\"WER - SNR 3 - wav2vec2 large-self: {wer_snr3_self}\")\n",
        "print(f\"CER - SNR 3 - wav2vec2 large-self: {cer_snr3_self}\")\n",
        "\n",
        "wer_snr6_self, cer_snr6_self = evaluate(model_self, processor_self, dataset_snr_6)\n",
        "print(f\"WER - SNR 6 - wav2vec2 large-self: {wer_snr6_self}\")\n",
        "print(f\"CER - SNR 6 - wav2vec2 large-self: {cer_snr6_self}\")\n",
        "\n",
        "wer_snr9_self, cer_snr9_self = evaluate(model_self, processor_self, dataset_snr_9)\n",
        "print(f\"WER - SNR 9 - wav2vec2 large-self: {wer_snr9_self}\")\n",
        "print(f\"CER - SNR 9 - wav2vec2 large-self: {cer_snr9_self}\")\n",
        "\n",
        "#  Wav2Vec2-Large-Self for single speaker clean dataset | speakers 1 and 2\n",
        "wer_clean_speaker1_self, cer_clean_speaker1_self = evaluate(model_self, processor_self, dataset_speaker_1)\n",
        "print(f\"WER - Clean speaker 1 - wav2vec2 large-self: {wer_clean_speaker1_self}\")\n",
        "print(f\"CER - Clean speaker 1 - wav2vec2 large-self: {cer_clean_speaker1_self}\")\n",
        "\n",
        "wer_clean_speaker2_self, cer_clean_speaker2_self = evaluate(model_self, processor_self, dataset_speaker_2)\n",
        "print(f\"WER - Clean speaker 2 - wav2vec2 large-self: {wer_clean_speaker2_self}\")\n",
        "print(f\"CER - Clean speaker 2 - wav2vec2 large-self: {cer_clean_speaker2_self}\")\n",
        "\n",
        "# WAV2VEC2-Large-Self for single speaker noisy (SNR 3) | speakers 1 and 2\n",
        "wer_noisy_speaker1_self, cer_noisy_speaker1_self = evaluate(model_self, processor_self, dataset_snr_3_speaker_1)\n",
        "print(f\"WER - Noisy SNR 3 speaker 1 - wav2vec2 large-self: {wer_noisy_speaker1_self}\")\n",
        "print(f\"CER - Noisy SNR 3 speaker 1 - wav2vec2 large-self: {cer_noisy_speaker1_self}\")\n",
        "\n",
        "wer_noisy_speaker2_self, cer_noisy_speaker2_self = evaluate(model_self, processor_self, dataset_snr_3_speaker_2)\n",
        "print(f\"WER - Noisy SNR 3 speaker 2 - wav2vec2 large-self: {wer_noisy_speaker2_self}\")\n",
        "print(f\"CER - Noisy SNR 3 speaker 2 - wav2vec2 large-self: {cer_noisy_speaker2_self}\")"
      ],
      "metadata": {
        "id": "67Tqq3LpSw6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part V: Experiments with wav2vec2** |\n",
        "ΤΩΡΑ ΞΕΚΙΝΑΩ ΝΑ ΒΛΕΠΩ ΑΚΡΙΒΩΣ ΤΑ ΛΑΘΗ ΣE 5 TRANSCRIPTIONS ΕΝΔΕΙΚΤΙΚΑ (ΟΧΙ ΟΛΑ ΛΟΓΩ ΧΡΟΝΟΥ ΚΑΙ ΥΠΟΛΟΓΙΣΤΙΚΟΥ ΒΑΡΟΥΣ)\n",
        "\n",
        "Υποερώτημα 3.Inspect the resulting transcriptions and comment on the errors (for each model and dataset)"
      ],
      "metadata": {
        "id": "5QPVTClkZJ50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Αποφάσισα να τρέξω εδώ ξανά τον κώδικα με τo result αυτην την φορά, ώστε να έχω ξεχωριστά τα metrics και τα transcriptions. Τώρα βλέπω ακριβώς τι λάθη (πρόβλεψη-στόχος) έγιναν σε καθε dataset ανά μοντέλο.\n",
        "def evaluate(model, processor, dataset):\n",
        "  model = model.to(\"cuda\")\n",
        "  model.eval()\n",
        "\n",
        "  def map_to_pred(batch):\n",
        "    \"\"\"\n",
        "    1. Pass batch[\"audio\"][\"array\"] through the processor and return\n",
        "      pytorch (pt) tensors. Get the input values of the resulting object.\n",
        "    2. Use with torch.no_grad() to disable gradients\n",
        "      Calculate the logits by passing the input values through the model\n",
        "    3. Get the predicted ids using argmax on the logits\n",
        "    4. Get the transcription by using processor.batch_decode\n",
        "    5. Set batch[\"transcription\"] to the transcription\n",
        "    \"\"\"\n",
        "    inputs = processor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(inputs.input_values.to(\"cuda\")).logits\n",
        "\n",
        "    predicted_ids = torch.argmax (logits, dim=-1) # ti einai to dim=1 xreiazetai ?\n",
        "    transcription = processor.batch_decode(predicted_ids)[0] # na dw is prsthikes\n",
        "    batch[\"transcription\"] = transcription\n",
        "    return batch\n",
        "\n",
        "    # DONE\n",
        "\n",
        "  result = dataset.map(map_to_pred, batched=False, remove_columns=[\"audio\"])\n",
        "  WER = calculate_wer(result[\"text\"], result[\"transcription\"])\n",
        "  CER = calculate_cer(result[\"text\"], result[\"transcription\"])\n",
        "  return WER, CER, result  # WANT TO RETURN NOW THE RESULT OF THE TRANSCRIPTIONS (GROUND TRUTH AND PREDICTION)"
      ],
      "metadata": {
        "id": "eaNRT2p56LCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC2-BASE for clean dataset\n",
        "wer_clean, cer_clean, result_clean = evaluate(model_base, processor_base, dataset_clean)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "NvOhAppwZS13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wav2vec2-base model shows very good performance on clean data, with most transcriptions matching the ground truth exactly. We may notice that the word “HARDWIGG” was transcribed as “HARDWIG,” which is a small truncation error. In Sample 4, we see “A” before “HIGHWAYMAN,” changing “BETTER FRIAR THAN HIGHWAYMAN” to “BETTER FRIAR THAN A HIGHWAYMAN,” which is a minor insertion. In general, these small errors are rare and do not significantly affect the overall intelligibility.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8bGFnMOE5CTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC2-BASE for SNR-3 noisy dataset\n",
        "wer_snr3_base, cer_snr3_base, result_snr3_base = evaluate(model_base, processor_base, dataset_snr_3)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr3_base[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr3_base[i][\"transcription\"])\n",
        "\n",
        "# WAV2VEC2-BASE for SNR-6 noisy dataset\n",
        "wer_snr6_base, cer_snr6_base, result_snr6_base = evaluate(model_base, processor_base, dataset_snr_6)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr6_base[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr6_base[i][\"transcription\"])\n",
        "\n",
        "# WAV2VEC2-BASE for SNR-9 noisy dataset\n",
        "wer_snr9_base, cer_snr9_base, result_snr9_base = evaluate(model_base, processor_base, dataset_snr_9)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr9_base[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr9_base[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "0ubK3Ifgg5IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transcriptions under noisy dataset in base model (SNR 3, SNR 6, and SNR 9) show better accuracy as noise decreases. At SNR 3, the model severe errors, including insertions, substitutions. For example, we see that \"KNIGHT ERRANT\" becomes \"MY LAWYER DOT\", showing significant distortion. Example 2: \"NO MISTER HARDWIGG\" is turned into \"OH MISTER HARDWICG\". At SNR 6, the model still struggles, but its performance is better. Example 3:  \"PENANCES\" is misrecognized as \"PENANCELS\", which is a substitution. At SNR 9, transcription quality improves further, with only minor distortions. Words like \"JOURNEY\" become \"JOURNEYY\", indicating an insertion, and \"PENANCES\" becomes \"TENNANCES\". Overall, the model handles low noise (SNR 9) fairly well, but under heavier noise (SNR 3) WER and CER show degradation."
      ],
      "metadata": {
        "id": "fbvYiKUw9rRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC2-BASE for single speaker clean dataset 1\n",
        "wer_clean_speaker1_base, cer_clean_speaker1_base, result_clean_speaker1_base = evaluate(model_base, processor_base, dataset_speaker_1)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_speaker1_base[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_speaker1_base[i][\"transcription\"])\n",
        "\n",
        "#for single speaker clean dataset 2\n",
        "wer_clean_speaker2_base, cer_clean_speaker2_base, result_clean_speaker2_base = evaluate(model_base, processor_base, dataset_speaker_2)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_speaker2_base[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_speaker2_base[i][\"transcription\"])\n",
        "\n",
        "# for single speaker 1 noisy (SNR 3)\n",
        "wer_noisy_speaker1_base, cer_noisy_speaker1_base, result_noisy_speaker1_base = evaluate(model_base, processor_base, dataset_snr_3_speaker_1)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_noisy_speaker1_base[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_noisy_speaker1_base[i][\"transcription\"])\n",
        "\n",
        "# for single speaker 2 noisy (SNR 3)\n",
        "wer_noisy_speaker2_base, cer_noisy_speaker2_base, result_noisy_speaker2_base = evaluate(model_base, processor_base, dataset_snr_3_speaker_2)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_noisy_speaker2_base[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_noisy_speaker2_base[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "gaHGn8Pmrp9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **clean conditions, both speaker 1 and 2** achieve high transcription accuracy with only small errors, such as rare name substitutions (“KALIKO” → “CALICO”) or character changes-cer (“IROLG” → “IROLD”).\n",
        "\n",
        "However, under **noisy SNR 3 conditions**, transcription quality drops sharply. The model does severe distortions, including incorrect word insertions. 1 Example: “THE STRENGTH...” becomes “STRENGTHEDIN NEHE SOG...”. This highlights the model’s strong performance on clean data but clear vulnerability to noise."
      ],
      "metadata": {
        "id": "Bvt3oc4zMfSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model large - Clean dataset\n",
        "wer_clean_large, cer_clean_large, result_clean_large = evaluate(model_large, processor_large, dataset_clean)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_large[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "_O2xB2ZFrqKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wav2vec2-large model shows excellent performance on clean speech with perfect transcriptions. There are minor changes between predicted and ground truth transcriptions such as “HARDWIGG” being shortened to “HARDWIG” and slight character changes (insertions, substitutions) like “GASCON” → “GASCONE” or “CATALAN” → “CATALIN.” These are low-impact substitutions and do not affect the overall meaning. Overall, the model handles clean audio with high accuracy."
      ],
      "metadata": {
        "id": "gSEObz-XQLQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC2-LARGE for SNR-3 noisy dataset\n",
        "wer_snr3_large, cer_snr3_large, result_snr3_large = evaluate(model_large, processor_large, dataset_snr_3)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr3_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr3_large[i][\"transcription\"])\n",
        "\n",
        "# WAV2VEC2-LARGE for SNR-6 noisy dataset\n",
        "wer_snr6_large, cer_snr6_large, result_snr6_large = evaluate(model_large, processor_large, dataset_snr_6)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr6_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr6_large[i][\"transcription\"])\n",
        "\n",
        "# WAV2VEC2-LARGE for SNR-9 noisy dataset\n",
        "wer_snr9_large, cer_snr9_large, result_snr9_large = evaluate(model_large, processor_large, dataset_snr_9)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr9_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr9_large[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "dc8tfPVarqXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At SNR 3 in large model, we can notice many errors in CER and WER but there are fewer errors than these of the base model snr3-snr6-snr9. Example: substitutions and meaningless phrases like “MY LAWYER GOT A CALLIN.” In SNR 6, accuracy improves but some distortions remain, such as “PREPER” instead of “COVER.” By SNR 9, the model performs well, with only minor mistakes like “CATALING” for “CATALAN.” Overall, as the SNR decreases, we notice more mistakes in hypothesis."
      ],
      "metadata": {
        "id": "TwoWSYZ9R6Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WAV2VEC2-LARGE for single speaker clean dataset 1\n",
        "wer_clean_speaker1_large, cer_clean_speaker1_large, result_clean_speaker1_large = evaluate(model_large, processor_large, dataset_speaker_1)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_speaker1_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_speaker1_large[i][\"transcription\"])\n",
        "\n",
        "# for single speaker clean dataset 2\n",
        "wer_clean_speaker2_large, cer_clean_speaker2_large, result_clean_speaker2_large = evaluate(model_large, processor_large, dataset_speaker_2)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_speaker2_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_speaker2_large[i][\"transcription\"])\n",
        "\n",
        "# for single speaker 1 noisy (SNR 3)\n",
        "wer_noisy_speaker1_large, cer_noisy_speaker1_large, result_noisy_speaker1_large = evaluate(model_large, processor_large, dataset_snr_3_speaker_1)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_noisy_speaker1_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_noisy_speaker1_large[i][\"transcription\"])\n",
        "\n",
        "# for single speaker 2 noisy (SNR 3)\n",
        "wer_noisy_speaker2_large, cer_noisy_speaker2_large, result_noisy_speaker2_large = evaluate(model_large, processor_large, dataset_snr_3_speaker_2)\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_noisy_speaker2_large[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_noisy_speaker2_large[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "Ou4Ed1NSruob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On **clean dataset,** the model performs very well for both speakers, with mostly perfect transcriptions and only minor errors such as phonetic substitutions like “KALIKO” → “CALIKO” or small deletions in characters (e.g., “BUTTE” → “BUTE”).\n",
        "\n",
        "In **SNR 3,** the transcription quality drops significantly. Both speakers have severe errors including substitutions of words, deletions etc. For example, “REPLIED KALIKO” becomes “REPLIED THE COWGO” and “GO QUIETLY ALONE...” turns into  “WEAWILIA O MOON...”. The model struggles with noise, especially in longer sentences.\n",
        "\n",
        "Overall, wav2vec2-large maintains high accuracy in clean speech but remains full of errros in heavy noise, particularly in single-speaker conditions."
      ],
      "metadata": {
        "id": "octfSV55To7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-large-self for clean datasetwer_noisy_speaker1_self, cer_noisy_speaker1_self = evaluate(model_self, processor_self, dataset_snr_3_speaker_1)\n",
        "wer_clean_self, cer_clean_self, result_clean_self = evaluate(model_self, processor_self, dataset_clean)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_self[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "OXy2HeMdrwb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model LARGE SELF TRAINING demonstrates exceptional transcription performance in clean dataset with minor errors in hard/rare words. Only minor deletions For example, “HARDWIGG” is transcribed as “HARDWIG”. The majority of hypotheses have no substantial errors. Overall, the model handles clean audio with high precision."
      ],
      "metadata": {
        "id": "60aadUMVYeCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-large-self for SNR dataste\n",
        "# SNR 3\n",
        "wer_snr3_self, cer_snr3_self, result_snr3_self = evaluate(model_self, processor_self, dataset_snr_3)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr3_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr3_self[i][\"transcription\"])\n",
        "\n",
        "# SNR 6\n",
        "wer_snr6_self, cer_snr6_self, result_snr6_self = evaluate(model_self, processor_self, dataset_snr_6)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr6_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr6_self[i][\"transcription\"])\n",
        "\n",
        "# SNR 9\n",
        "wer_snr9_self, cer_snr9_self, result_snr9_self = evaluate(model_self, processor_self, dataset_snr_9)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_snr9_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_snr9_self[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "ojFqGNsnwzCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At **SNR 3**, the wav2vec2-large self-training model makes errors without sense like deletions and substitutions. EXAMPLE: 'HOW TO' becomes 'A GUN'. At **SNR 6**, accuracy improves with some errors remaining 'AND CATALAN' - 'MACATALY' that do not worsen the meaning. By **SNR 9**, the model produces almost perfect transcriptions, with only minimal differences from the ground truth. Overall, the performance of this model in CER and WER is even better compared to the previous models."
      ],
      "metadata": {
        "id": "FTE1CRkiZYBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-large-self for single speakers dataset\n",
        "# Clean speaker 1\n",
        "wer_clean_speaker1_self, cer_clean_speaker1_self, result_clean_speaker1_self = evaluate(model_self, processor_self, dataset_speaker_1)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_speaker1_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_speaker1_self[i][\"transcription\"])\n",
        "\n",
        "# Clean speaker 2\n",
        "wer_clean_speaker2_self, cer_clean_speaker2_self, result_clean_speaker2_self = evaluate(model_self, processor_self, dataset_speaker_2)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_clean_speaker2_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_clean_speaker2_self[i][\"transcription\"])\n",
        "\n",
        "# Noisy speaker 1 (SNR 3)\n",
        "wer_noisy_speaker1_self, cer_noisy_speaker1_self, result_noisy_speaker1_self = evaluate(model_self, processor_self, dataset_snr_3_speaker_1)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_noisy_speaker1_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_noisy_speaker1_self[i][\"transcription\"])\n",
        "\n",
        "# Noisy speaker 2 (SNR 3)\n",
        "wer_noisy_speaker2_self, cer_noisy_speaker2_self, result_noisy_speaker2_self = evaluate(model_self, processor_self, dataset_snr_3_speaker_2)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"\\n-Sample {i+1}\")\n",
        "    print(\"Ground truth   :\", result_noisy_speaker2_self[i][\"text\"])\n",
        "    print(\"Predicted text :\", result_noisy_speaker2_self[i][\"transcription\"])"
      ],
      "metadata": {
        "id": "U2I4sQJkwzsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the clean condition single speaker, the model performs very well, with near-perfect transcriptions. The are low-impact errors such as phonetic substitutions like “KALIKO” → “CALICO” or “LASSON’S” for “LASSEN’S.”.\n",
        "\n",
        "Under SNR 3- single speaker, performance declines with increased substitutions, insertions, and distortions. Errors such as “STRENGTH OF THE KNEE” for “THE STRENGTH THAT ENABLES” and “THE MITTER FIRST” instead of “THE METAL FOREST” demonstrate confusion in meaning. However, the performance of this model here is even better compared to the previous models."
      ],
      "metadata": {
        "id": "SEYy3ZCVglaH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}